apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
  # Хоть и не было сказано, решил указать namespace для дальнейшей изоляции
  namespace: development 
  labels:
    app: web-app
spec:
  # Ставим 2 реплики Pod, количество будет менятся при помощи HPA в зависимости от нагрузки(указано далее в манифесте)
  replicas: 2 
  strategy:
    type: RollingUpdate # RollingUpdate для плавного обновления
    rollingUpdate:
      maxUnavailable: 1 # Поставил 1, чтобы при обновлении не требовалось больше ресурсов
      maxSurge: 1 # Поднимаем по одному новому Pod
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      topologySpreadConstraints: # SpreadConstraints для распределения по зонам
        - maxSkew: 1
          topologyKey: "topology.kubernetes.io/zone"
          whenUnsatisfiable: ScheduleAnyway # Мягкое ограничение
          labelSelector:
            matchLabels:
              app: web-app
      affinity:
        podAntiAffinity: # Используем AntiAffinity для размещения по нодам
          preferredDuringSchedulingIgnoredDuringExecution: # Ставим не жесткие ограничения
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: web-app
              topologyKey: "kubernetes.io/hostname" # Новые поды будут стараться размещаться на нодах, где еще нет подов
      containers:
      - name: web-app
        # Образ берем с указанием registry, если используется локальный
        image: company.registry/web-app:v1.0.1 # Явно указан тег для предсказуемости и контроля
        ports: # Настраиваются в соответствии с приложением и политикой компании
        - name: liveness-port
          containerPort: 8080
        resources:
          requests: # Минимальный запрос на ресурсы 
            cpu: "100m" # В среднем приложению нужно 100m
            memory: 128Mi # Аналогично для памяти 128Mi
          limits: # Лимит по процессору не ставим
            memory: 256Mi
        startupProbe: # Probe для проверки успешности старта контейнера
          httpGet: # Настраивается в зависимости от приложения
            path: /healthz
            port: liveness-port
          periodSeconds: 2
          failureThreshold: 10
        readinessProbe: # Probe для проверки успешности инициализации приложения
          httpGet:
            path: /healthz
            port: liveness-port
          initialDelaySeconds: 5 # Задержка в 5 секунд перед проверкой(по условию инициализация требует 5-10с)
          periodSeconds: 3
          failureThreshold: 5 # В худшем случае после запуска на проверку старта и инициализации уйдет 40 секунд
        livenessProbe: # Probe для постоянной проверки жизнеспособности контейнера после запуска
          httpGet: # Настраивается в зависимости от приложения
            path: /healthz
            port: liveness-port
          periodSeconds: 10 # Проверка каждые 10 секунд (default)
          failureThreshold: 5
          # restarPolicy не задавал, т.к. нужно Always(стоит по умолчанию)

---
# В задании было указано написать решение в одном манифесте, поэтому здесь же описывается HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler # Используем HPA для горизонтального автоскейлинга
metadata:
  name: app-hpa
  namespace: development
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  minReplicas: 2 # 1 реплики было бы достаточно для нагрузки ночью, но поставил 2 для повышения отказоустойчивости
  maxReplicas: 5 # Хоть и 4 пода справляются с пиковой нагрузкой, поставил 5 для возможных всплесков (тем более, что 5 нод)
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        targetAverageUtilization: 100 # Приводим к среднему использованию процессора - 100m
    # Решил не отслеживать по памяти, т.к. она "всегда" постоянна
  behavior: # Дополнительные настройки политик скейлинга
    scaleUp:
      stabilizationWindowSeconds: 60 # ВременнОе окно для того, чтобы убедиться, что нагрузка постоянно возросла. \
      # \ Надеюсь, это решает проблему с ложным автоскейлингом во время возрастания нагрузки на CPU при первых запросах. \
      # \ В ином случае, предложил бы переместить в отдельный файл \
      # \ и при применении манифеста добавлять ключ --horizontal-pod-autoscaler-initial-readiness-delay
      policies:
      - type: Pods
        value: 2 # Ограничил максимальное количество поднимаемых подов в минуту с расчетом, что нагрузка растет не резко
        periodSeconds: 60
        # selectPolicy не ставлю, т.к. кластер небольшой -> в процентах нет смысла указывать, поэтому только одна политика
    scaleDown:
      stabilizationWindowSeconds: 300 # 5 минут, чтобы убедиться, что нагрузка точно снизилась
      policies:
      - type: Pods
        value: 1 # Указан 1 под в минуту для избежания резкого падения производительности
        periodSeconds: 60
